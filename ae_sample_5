import os
import re
import yaml
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from enum import Enum
import faiss
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import dateparser
import logging
import warnings
import requests

import gspread
from google.auth import default

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IntentType(Enum):
    FETCH_METRIC = "fetch_metric"
    COMPARE_METRIC = "compare_metric"
    RANK_ENTITIES = "rank_entities"
    DIAGNOSE_METRIC = "diagnose_metric"
    TREND_ANALYSIS = "trend_analysis"
    THRESHOLD_CHECK = "threshold_check"
    LIST_ENTITIES_BY_CRITERIA = "list_entities_by_criteria"
    SUMMARIZE_METRIC = "summarize_metric"
    GET_RECOMMENDATION = "get_recommendation"

@dataclass
class TimeFrame:
    expression: str
    start: str
    end: str

@dataclass
class ExtractedContext:
    entities: Dict[str, Any]
    timeframe: Optional[TimeFrame] = None
    filters: Dict[str, Any] = field(default_factory=dict)
    metrics: List[str] = field(default_factory=list)

@dataclass
class QueryResult:
    intent: IntentType
    user_query: str
    extracted_context: ExtractedContext
    similar_contexts: List[Dict[str, Any]] = field(default_factory=list)
    confidence: float = 0.0

@dataclass
class AppState:
    user_query: str = ""
    query_result: Optional[QueryResult] = None
    raw_data: Optional[pd.DataFrame] = None
    filtered_data: Optional[pd.DataFrame] = None
    analysis_results: Dict[str, Any] = field(default_factory=dict)
    anomalies: List[Dict[str, Any]] = field(default_factory=list)
    report: str = ""
    recommendations: List[str] = field(default_factory=list)
    error: Optional[str] = None
    next_suggested_questions: List[str] = field(default_factory=list)

class SemanticLayerManager:
    def __init__(self, yaml_file_path: str):
        self.semantic_layer = self._load_semantic_layer(yaml_file_path)
        self.column_mappings = self._build_column_mappings()
        self.metrics_definitions = self._extract_metrics_definitions()
        
    def _load_semantic_layer(self, yaml_file_path: str) -> Dict:
        try:
            with open(yaml_file_path, 'r') as file:
                return yaml.safe_load(file)
        except FileNotFoundError:
            logger.error(f"Semantic layer file not found: {yaml_file_path}")
            return self._get_default_semantic_layer()
    
    def _get_default_semantic_layer(self) -> Dict:
        return {
            "tables": {
                "efficiency_table": {
                    "description": "Main table for efficiency and operational analysis",
                    "columns": {
                        "vehicle_id": {
                            "description": "Unique vehicle identifier",
                            "type": "string",
                            "semantic": ["VID", "vehicle_id", "vehicle", "id"]
                        },
                        "country_code": {
                            "description": "Country code",
                            "type": "string",
                            "semantic": ["country", "country_code", "nation"]
                        },
                        "region": {
                            "description": "Geographic region",
                            "type": "string",
                            "semantic": ["region"]
                        },
                        "d_date": {
                            "description": "Date of drive (YYYY-MM-DD)",
                            "type": "date",
                            "semantic": ["drive_date", "date", "d_date"]
                        },
                        "d_month": {
                            "description": "Month of drive (YYYY-MM-01)",
                            "type": "date",
                            "semantic": ["drive_month", "month"]
                        },
                        "Targetdistance": {
                            "description": "Target kilometers",
                            "type": "float",
                            "semantic": ["TKM", "target_km", "target_kilometers", "Target distance"]
                        },
                        "Totaldistance": {
                            "description": "Total kilometers",
                            "type": "float",
                            "semantic": ["Total_KM", "total_km", "total_kilometers"]
                        },
                        "Hours": {
                            "description": "Hours logged",
                            "type": "float",
                            "semantic": ["SOH", "hours_logged", "state_of_hours"]
                        },
                        "season_name": {
                            "description": "Season",
                            "type": "string",
                            "semantic": ["season", "season_name", "season year"]
                        },
                        "user_email": {
                            "description": "Driver/user email",
                            "type": "string",
                            "semantic": ["user_email", "driver_email", "email"]
                        }
                    }
                }
            },
            "metrics": {
                "efficiency": {
                    "description": "Efficiency as Targetdistance divided by Totaldistance",
                    "formula": "SUM(Targetdistance)/NULLIF(SUM(Totaldistance),0)",
                    "constraints": ["Totaldistance > 0"],
                    "business_rules": ["Flag if efficiency < 0.2", "Efficiency must be between 0 and 2"]
                }
            }
        }
    
    def _build_column_mappings(self) -> Dict[str, str]:
        mappings = {}
        for table_name, table_info in self.semantic_layer.get("tables", {}).items():
            for column_name, column_info in table_info.get("columns", {}).items():
                semantic_terms = column_info.get("semantic", [])
                for term in semantic_terms:
                    mappings[term.lower()] = column_name
                mappings[column_name.lower()] = column_name
        return mappings
    
    def _extract_metrics_definitions(self) -> Dict[str, Dict]:
        return self.semantic_layer.get("metrics", {})
    
    def resolve_column_name(self, semantic_term: str) -> Optional[str]:
        return self.column_mappings.get(semantic_term.lower())
    
    def get_metric_definition(self, metric_name: str) -> Optional[Dict]:
        return self.metrics_definitions.get(metric_name.lower())

class TimeExpressionParser:
    def __init__(self, reference_date: str = "2025-07-14"):
        self.reference_date = datetime.strptime(reference_date, "%Y-%m-%d")
        self.time_expressions = self._build_time_expressions()
    
    def _build_time_expressions(self) -> Dict[str, Tuple[str, str]]:
        ref_date = self.reference_date
        week_start = ref_date - timedelta(days=ref_date.weekday())
        last_week_start = week_start - timedelta(days=7)
        month_start = ref_date.replace(day=1)
        last_month_end = month_start - timedelta(days=1)
        last_month_start = last_month_end.replace(day=1)
        quarter_month = ((ref_date.month - 1) // 3) * 3 + 1
        quarter_start = ref_date.replace(month=quarter_month, day=1)
        last_quarter_end = quarter_start - timedelta(days=1)
        last_quarter_start = (last_quarter_end.replace(day=1) - timedelta(days=62)).replace(day=1)
        return {
            "this week": (week_start.strftime("%Y-%m-%d"), (week_start + timedelta(days=6)).strftime("%Y-%m-%d")),
            "last week": (last_week_start.strftime("%Y-%m-%d"), (last_week_start + timedelta(days=6)).strftime("%Y-%m-%d")),
            "this month": (month_start.strftime("%Y-%m-%d"), ref_date.strftime("%Y-%m-%d")),
            "last month": (last_month_start.strftime("%Y-%m-%d"), last_month_end.strftime("%Y-%m-%d")),
            "this year": (ref_date.replace(month=1, day=1).strftime("%Y-%m-%d"), ref_date.strftime("%Y-%m-%d")),
            "last year": ((ref_date.replace(year=ref_date.year-1, month=1, day=1)).strftime("%Y-%m-%d"), 
                         (ref_date.replace(year=ref_date.year-1, month=12, day=31)).strftime("%Y-%m-%d")),
            "this quarter": (quarter_start.strftime("%Y-%m-%d"), ref_date.strftime("%Y-%m-%d")),
            "last quarter": (last_quarter_start.strftime("%Y-%m-%d"), last_quarter_end.strftime("%Y-%m-%d")),
            "last 6 months": ((ref_date - timedelta(days=180)).strftime("%Y-%m-%d"), ref_date.strftime("%Y-%m-%d")),
            "last 2 months": ((ref_date - timedelta(days=60)).strftime("%Y-%m-%d"), ref_date.strftime("%Y-%m-%d")),
            "till now": ("1970-01-01", ref_date.strftime("%Y-%m-%d")),
            "till date": ("1970-01-01", ref_date.strftime("%Y-%m-%d")),
            "as of yesterday": ("1970-01-01", (ref_date - timedelta(days=1)).strftime("%Y-%m-%d"))
        }
    
    def parse_time_expression(self, expression: str) -> Optional[TimeFrame]:
        expression = expression.lower().strip()
        if expression in self.time_expressions:
            start, end = self.time_expressions[expression]
            return TimeFrame(expression=expression, start=start, end=end)
        week_pattern = r"week of (\d+)(?:st|nd|rd|th)?\s+(\w+)"
        match = re.search(week_pattern, expression)
        if match:
            day, month = match.groups()
            return self._parse_week_of_date(day, month)
        nth_week_pattern = r"(\d+)(?:st|nd|rd|th)?\s+week of (\w+)"
        match = re.search(nth_week_pattern, expression)
        if match:
            week_num, month = match.groups()
            return self._parse_nth_week_of_month(week_num, month)
        try:
            parsed_date = dateparser.parse(expression)
            if parsed_date:
                return TimeFrame(
                    expression=expression,
                    start=parsed_date.strftime("%Y-%m-%d"),
                    end=parsed_date.strftime("%Y-%m-%d")
                )
        except Exception as e:
            logger.warning(f"Failed to parse time expression '{expression}': {e}")
        return None
    
    def _parse_week_of_date(self, day: str, month: str) -> Optional[TimeFrame]:
        try:
            month_num = self._month_name_to_number(month)
            if month_num is None:
                return None
            target_date = datetime(self.reference_date.year, month_num, int(day))
            week_start = target_date - timedelta(days=target_date.weekday())
            week_end = week_start + timedelta(days=6)
            return TimeFrame(
                expression=f"week of {day} {month}",
                start=week_start.strftime("%Y-%m-%d"),
                end=week_end.strftime("%Y-%m-%d")
            )
        except Exception:
            return None
    
    def _parse_nth_week_of_month(self, week_num: str, month: str) -> Optional[TimeFrame]:
        try:
            month_num = self._month_name_to_number(month)
            if month_num is None:
                return None
            first_day = datetime(self.reference_date.year, month_num, 1)
            days_to_monday = (7 - first_day.weekday()) % 7
            first_monday = first_day + timedelta(days=days_to_monday)
            week_start = first_monday + timedelta(weeks=int(week_num) - 1)
            week_end = week_start + timedelta(days=6)
            return TimeFrame(
                expression=f"{week_num} week of {month}",
                start=week_start.strftime("%Y-%m-%d"),
                end=week_end.strftime("%Y-%m-%d")
            )
        except Exception:
            return None
    
    def _month_name_to_number(self, month_name: str) -> Optional[int]:
        months = {
            'january': 1, 'jan': 1, 'february': 2, 'feb': 2, 'march': 3, 'mar': 3,
            'april': 4, 'apr': 4, 'may': 5, 'june': 6, 'jun': 6, 'july': 7, 'jul': 7,
            'august': 8, 'aug': 8, 'september': 9, 'sep': 9, 'october': 10, 'oct': 10,
            'november': 11, 'nov': 11, 'december': 12, 'dec': 12
        }
        return months.get(month_name.lower())

# --- MINIMAL CHANGE: VectorSearchManager now uses FAISS HNSW for similarity search ---
class VectorSearchManager:
    def __init__(self):
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        self.query_history = []
        self.index = None
        self.dimension = 0
        self.vectors = None

    def add_query(self, query: str, intent: str, context: dict):
        self.query_history.append({
            "query": query,
            "intent": intent,
            "context": context,
            "timestamp": datetime.now().isoformat()
        })
        self._rebuild_index()

    def _rebuild_index(self):
        if len(self.query_history) < 2:
            self.index = None
            return
        queries = [item["query"] for item in self.query_history]
        self.vectors = self.vectorizer.fit_transform(queries).toarray().astype('float32')
        self.dimension = self.vectors.shape[1]
        faiss.normalize_L2(self.vectors)
        self.index = faiss.IndexHNSWFlat(self.dimension, 32)
        self.index.hnsw.efConstruction = 40
        self.index.add(self.vectors)

    def search_similar_queries(self, query: str, top_k: int = 3):
        if self.index is None or len(self.query_history) == 0:
            return []
        query_vector = self.vectorizer.transform([query]).toarray().astype('float32')
        faiss.normalize_L2(query_vector)
        self.index.hnsw.efSearch = 16
        scores, indices = self.index.search(query_vector, min(top_k, len(self.query_history)))
        similar_queries = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.query_history) and score > 0.1:
                similar_queries.append({
                    "query": self.query_history[idx]["query"],
                    "intent": self.query_history[idx]["intent"],
                    "similarity": float(score),
                    "context": self.query_history[idx]["context"]
                })
        return similar_queries
# --- END MINIMAL CHANGE ---

class QueryUnderstandingAgent:
    def __init__(self, gemini_api_key: str, semantic_layer_manager: SemanticLayerManager):
        self.gemini_api_key = gemini_api_key
        self.semantic_layer = semantic_layer_manager
        self.time_parser = TimeExpressionParser()
        self.vector_search = VectorSearchManager()
        self.gemini_api_url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent"

    def process_query(self, query: str) -> QueryResult:
        try:
            intent = self._extract_intent(query)
            extracted_context = self._extract_entities_and_context(query)
            similar_contexts = self.vector_search.search_similar_queries(query)
            result = QueryResult(
                intent=intent,
                user_query=query,
                extracted_context=extracted_context,
                similar_contexts=similar_contexts,
                confidence=0.8
            )
            self.vector_search.add_query(query, intent.value, extracted_context.__dict__)
            return result
        except Exception as e:
            logger.error(f"Error processing query: {e}")
            raise

    def _extract_intent(self, query: str) -> IntentType:
        intent_examples = {
            "fetch_metric": ["show", "get", "what is", "display", "retrieve"],
            "compare_metric": ["compare", "versus", "vs", "difference", "against"],
            "rank_entities": ["rank", "top", "bottom", "best", "worst", "list"],
            "diagnose_metric": ["why", "reason", "cause", "diagnose", "explain"],
            "trend_analysis": ["trend", "pattern", "over time", "timeline", "progression"],
            "threshold_check": ["above", "below", "exceeds", "meets", "threshold"],
            "list_entities_by_criteria": ["which", "filter", "criteria", "condition"],
            "summarize_metric": ["summary", "summarize", "average", "mean", "total"],
            "get_recommendation": ["recommend", "suggestion", "improve", "how to", "advice"]
        }
        query_lower = query.lower()
        intent_scores = {}
        for intent_name, keywords in intent_examples.items():
            score = sum(1 for keyword in keywords if keyword in query_lower)
            if score > 0:
                intent_scores[intent_name] = score
        if intent_scores:
            best_intent = max(intent_scores, key=intent_scores.get)
            return IntentType(best_intent)
        return self._extract_intent_with_gemini(query)

    def _extract_intent_with_gemini(self, query: str) -> IntentType:
        prompt = (
            "Analyze the following query and determine the intent. Choose ONE from these options:\n"
            "- fetch_metric: Get a specific metric value\n"
            "- compare_metric: Compare metrics across entities/time\n"
            "- rank_entities: Rank or list entities by performance\n"
            "- diagnose_metric: Analyze reasons for metric values\n"
            "- trend_analysis: Show trends over time\n"
            "- threshold_check: Check if metrics meet thresholds\n"
            "- list_entities_by_criteria: List entities meeting criteria\n"
            "- summarize_metric: Provide summary statistics\n"
            "- get_recommendation: Get improvement recommendations\n"
            f"\nQuery: \"{query}\"\n\nRespond with only the intent name."
        )
        headers = {"Content-Type": "application/json"}
        params = {"key": self.gemini_api_key}
        data = {
            "contents": [{
                "parts": [{"text": prompt}]
            }]
        }
        try:
            r = requests.post(self.gemini_api_url, headers=headers, params=params, json=data)
            r.raise_for_status()
            response_txt = r.json()["candidates"][0]["content"]["parts"][0]["text"].strip()
            return IntentType(response_txt)
        except Exception as e:
            logger.warning(f"Gemini intent extraction failed: {e}")
            return IntentType.FETCH_METRIC

    def _extract_entities_and_context(self, query: str) -> ExtractedContext:
        entities = {}
        filters = {}
        metrics = []
        timeframe = None
        timeframe = self._extract_timeframe(query)
        query_lower = query.lower()
        if any(metric_term in query_lower for metric_term in ['efficiency', 'performance', 'ratio']):
            metrics.append('efficiency')
            entities['metric'] = 'efficiency'
        for semantic_term, column_name in self.semantic_layer.column_mappings.items():
            if semantic_term in query_lower:
                pattern = rf"{semantic_term}[:\s]+([A-Za-z0-9_-]+)"
                matches = re.findall(pattern, query_lower)
                if matches:
                    filters[column_name] = matches[0]
        vid_pattern = r"VID\s+(\d+|[A-Za-z0-9_-]+)"
        vid_matches = re.findall(vid_pattern, query, re.IGNORECASE)
        if vid_matches:
            filters['vehicle_id'] = vid_matches[0]
        region_pattern = r"region\s+([A-Za-z0-9_-]+)"
        region_matches = re.findall(region_pattern, query, re.IGNORECASE)
        if region_matches:
            filters['region'] = region_matches[0]
        return ExtractedContext(
            entities=entities,
            timeframe=timeframe,
            filters=filters,
            metrics=metrics
        )

    def _extract_timeframe(self, query: str) -> Optional[TimeFrame]:
        time_expressions = [
            'this week', 'last week', 'this month', 'last month',
            'this year', 'last year', 'this quarter', 'last quarter',
            'last 6 months', 'last 2 months', 'till now', 'till date',
            'as of yesterday'
        ]
        query_lower = query.lower()
        for expr in time_expressions:
            if expr in query_lower:
                return self.time_parser.parse_time_expression(expr)
        date_patterns = [
            r'week of (\d+)(?:st|nd|rd|th)?\s+(\w+)',
            r'(\d+)(?:st|nd|rd|th)?\s+week of (\w+)',
            r'in (\w+) (\d{4})',
            r'(\w+) (\d{4})'
        ]
        for pattern in date_patterns:
            match = re.search(pattern, query_lower)
            if match:
                matched_expr = match.group(0)
                return self.time_parser.parse_time_expression(matched_expr)
        return None

class DataRetrievalAgent:
    def __init__(self, semantic_layer_manager: SemanticLayerManager, gsheet_url: str):
        self.semantic_layer = semantic_layer_manager
        self.gsheet_url = gsheet_url
        self.data_cache = None
        self.gc = None
        self.sheet = None
        self._authenticate()
    
    def _authenticate(self):
        try:
            creds, _ = default(scopes=["https://www.googleapis.com/auth/spreadsheets.readonly"])
            self.gc = gspread.authorize(creds)
            self.sheet = self.gc.open_by_url(self.gsheet_url).sheet1
        except Exception as e:
            logger.error(f"Failed to authenticate or open Google Sheet: {e}")
            raise
    
    def retrieve_data(self, csv_file_path: str, query_result: QueryResult) -> pd.DataFrame:
        if self.data_cache is None:
            self.data_cache = self._load_gsheet_data()
        df = self.data_cache.copy()
        filtered_df = self._apply_filters(df, query_result.extracted_context)
        if query_result.extracted_context.timeframe:
            filtered_df = self._apply_time_filter(filtered_df, query_result.extracted_context.timeframe)
        return filtered_df
    
    def _load_gsheet_data(self) -> pd.DataFrame:
        try:
            records = self.sheet.get_all_records()
            df = pd.DataFrame(records)
            df = self._clean_data(df)
            df = self._validate_data(df)
            return df
        except Exception as e:
            logger.error(f"Error loading Google Sheet data: {e}")
            raise
    
    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        date_columns = ['d_date', 'd_month']
        for col in date_columns:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
        numeric_columns = ['Targetdistance', 'Totaldistance', 'Hours']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        df = df.dropna(subset=['Targetdistance', 'Totaldistance'])
        df = df[df['Totaldistance'] > 0]
        return df
    
    def _validate_data(self, df: pd.DataFrame) -> pd.DataFrame:
        df['efficiency'] = df['Targetdistance'] / df['Totaldistance']
        df = df[df['efficiency'] >= 0]
        df = df[df['efficiency'] <= 2]
        return df
    
    def _apply_filters(self, df: pd.DataFrame, context: ExtractedContext) -> pd.DataFrame:
        filtered_df = df.copy()
        for filter_key, filter_value in context.filters.items():
            column_name = self.semantic_layer.resolve_column_name(filter_key)
            if column_name and column_name in filtered_df.columns:
                filtered_df = filtered_df[filtered_df[column_name] == filter_value]
        return filtered_df
    
    def _apply_time_filter(self, df: pd.DataFrame, timeframe: TimeFrame) -> pd.DataFrame:
        if 'd_date' not in df.columns:
            return df
        start_date = pd.to_datetime(timeframe.start)
        end_date = pd.to_datetime(timeframe.end)
        return df[(df['d_date'] >= start_date) & (df['d_date'] <= end_date)]

# Paste your original AnalysisAgent, AnomalyDetectionAgent, and ReportGenerationAgent classes here, unchanged.

class ManagerAgent:
    def __init__(self, gemini_api_key, semantic_layer_yaml_path, gsheet_url):
        self.semantic_layer_manager = SemanticLayerManager(semantic_layer_yaml_path)
        self.query_agent = QueryUnderstandingAgent(gemini_api_key, self.semantic_layer_manager)
        self.data_agent = DataRetrievalAgent(self.semantic_layer_manager, gsheet_url)
        self.analysis_agent = AnalysisAgent(self.semantic_layer_manager)
        self.anomaly_agent = AnomalyDetectionAgent()
        self.report_agent = ReportGenerationAgent()
        self.gsheet_url = gsheet_url

    def run(self, user_query):
        query_result = self.query_agent.process_query(user_query)
        filtered_data = self.data_agent.retrieve_data(None, query_result)
        analysis_results = self.analysis_agent.analyze_data(filtered_data, query_result)
        anomalies = self.anomaly_agent.detect_anomalies(filtered_data)
        report = self.report_agent.generate_report(query_result, analysis_results, anomalies)
        return report

def build_state_graph(manager_agent):
    sg = StateGraph("ae_state_graph")
    sg.add_node("query_understanding", manager_agent.query_agent.process_query)
    sg.add_node("data_retrieval", manager_agent.data_agent.retrieve_data)
    sg.add_node("analysis", manager_agent.analysis_agent.analyze_data)
    sg.add_node("anomaly_detection", manager_agent.anomaly_agent.detect_anomalies)
    sg.add_node("report_generation", manager_agent.report_agent.generate_report)
    sg.add_edge("query_understanding", "data_retrieval")
    sg.add_edge("data_retrieval", "analysis")
    sg.add_edge("analysis", "anomaly_detection")
    sg.add_edge("anomaly_detection", "report_generation")
    sg.add_edge("report_generation", END)
    sg.set_entry_node("query_understanding")
    return sg

# Usage:
# manager = ManagerAgent(
#     gemini_api_key="YOUR_GEMINI_API_KEY",
#     semantic_layer_yaml_path="semantic_layer.yaml",
#     gsheet_url="YOUR_GOOGLE_SHEET_URL"
# )
# report = manager.run("Show efficiency for the last quarter by region.")
# print(report)
